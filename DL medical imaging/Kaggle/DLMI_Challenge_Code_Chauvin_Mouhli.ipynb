{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsMm1lU17Gq5"
      },
      "outputs": [],
      "source": [
        "# Torch modules\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Matplotlib modules\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "\n",
        "# numpy and pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Common python modules\n",
        "import os\n",
        "import re\n",
        "import datetime\n",
        "import sys\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inytHgDg7Mep"
      },
      "outputs": [],
      "source": [
        "# Import the data\n",
        "!git clone https://github.com/soniamartinot/MVA-Dose-Prediction.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9vrP8CI7ftp"
      },
      "outputs": [],
      "source": [
        "train_dir='./MVA-Dose-Prediction/train/'\n",
        "test_dir='./MVA-Dose-Prediction/test/'\n",
        "val_dir='./MVA-Dose-Prediction/validation/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv0dCHJE7jlA"
      },
      "outputs": [],
      "source": [
        "# Creation of the train, test and validation dataset\n",
        "\n",
        "class DoseDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,data_path):\n",
        "    self.data_path=data_path\n",
        "    self.samples=sorted(os.listdir(data_path),key=lambda s:int(re.search(r'\\d+',s).group()))\n",
        "  \n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "        sample_path = self.data_path + os.sep + self.samples[idx]\n",
        "        ct_scan_np=np.load(sample_path + os.sep + 'ct.npy')\n",
        "        possible_dose_mask_np=np.load(sample_path + os.sep + 'possible_dose_mask.npy')\n",
        "        structure_masks_np=np.load(sample_path + os.sep + 'structure_masks.npy')\n",
        "\n",
        "        ct_scan_np=np.multiply(ct_scan_np,possible_dose_mask_np)  #We apply the possible_dose_mask to the ct_scan\n",
        "\n",
        "        for i in range(10):\n",
        "          structure_masks_np[i]=np.multiply(ct_scan_np,structure_masks_np[i])       #We apply each mask on the new ct_scan\n",
        "\n",
        "        ct_scan = torch.from_numpy(ct_scan_np).float().unsqueeze(0)\n",
        "        possible_dose_mask = torch.from_numpy(possible_dose_mask_np).float().unsqueeze(0)\n",
        "        dose = torch.from_numpy(np.load(sample_path + os.sep + 'dose.npy')).float().unsqueeze(0)\n",
        "        structure_masks = torch.from_numpy(structure_masks_np).float()\n",
        "\n",
        "        inp=torch.cat((ct_scan,structure_masks,possible_dose_mask),0).float()\n",
        "\n",
        "        return inp, dose\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "class DoseDatasetTest(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self,data_path):\n",
        "    self.data_path=data_path\n",
        "    self.samples=sorted(os.listdir(data_path),key=lambda s:int(re.search(r'\\d+',s).group()))\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "        sample_path = self.data_path + os.sep + self.samples[idx]\n",
        "        ct_scan_np=np.load(sample_path + os.sep + 'ct.npy')\n",
        "        possible_dose_mask_np=np.load(sample_path + os.sep + 'possible_dose_mask.npy')\n",
        "        structure_masks_np=np.load(sample_path + os.sep + 'structure_masks.npy')\n",
        "\n",
        "        ct_scan_np=np.multiply(ct_scan_np,possible_dose_mask_np)\n",
        "\n",
        "        for i in range(10):\n",
        "          structure_masks_np[i]=np.multiply(ct_scan_np,structure_masks_np[i])\n",
        "\n",
        "        ct_scan = torch.from_numpy(ct_scan_np).float().unsqueeze(0)\n",
        "        possible_dose_mask = torch.from_numpy(possible_dose_mask_np).float().unsqueeze(0)\n",
        "        structure_masks = torch.from_numpy(structure_masks_np).float()\n",
        "\n",
        "        inp=torch.cat((ct_scan,structure_masks,possible_dose_mask),0).float()\n",
        "        \n",
        "        return inp\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN5OwBwv8Kf6"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "train_dataset = DoseDataset(train_dir)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "\n",
        "\n",
        "val_dataset = DoseDataset(val_dir)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n",
        "\n",
        "# Dataset test for the Codalab test\n",
        "\n",
        "test_dataset=DoseDatasetTest(test_dir)\n",
        "dataset_test=DataLoader(test_dataset,batch_size=1,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5VdOBHH7-7i"
      },
      "outputs": [],
      "source": [
        "class Squeeze_Excite(nn.Module):\n",
        "    \n",
        "    def __init__(self,channel,reduction):\n",
        "        super().__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.lin1 = nn.Linear(channel, channel // reduction, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)   # inplace = True decreases memory usage \n",
        "        self.lin2 = nn.Linear(channel // reduction, channel, bias=False)\n",
        "        self.sig = nn.Sigmoid()\n",
        " \n",
        "\n",
        "        \n",
        "    def forward(self,x):\n",
        "        b, c, _, _ = x.size()\n",
        "        out = self.avgpool(x).view(b, c)\n",
        "        out = self.lin1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.lin2(out)\n",
        "        out = self.sig(out).view(b,c,1,1)\n",
        "        return x * out.expand_as(x)  #Resizing\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tPJMhNXpYvI"
      },
      "outputs": [],
      "source": [
        "class VGGBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
        "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.SE = Squeeze_Excite(out_channels,reduction = 8)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.SE(out)\n",
        "      \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlmk8p1g71rH"
      },
      "outputs": [],
      "source": [
        "class Unetplus(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_channels=12, output_size=1, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        nb_channels = [64, 128, 256, 512, 1024]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv0_0 = VGGBlock(input_channels, nb_channels[0], nb_channels[0])\n",
        "        self.conv1_0 = VGGBlock(nb_channels[0], nb_channels[1], nb_channels[1])\n",
        "        self.conv2_0 = VGGBlock(nb_channels[1], nb_channels[2], nb_channels[2])\n",
        "        self.conv3_0 = VGGBlock(nb_channels[2], nb_channels[3], nb_channels[3])\n",
        "        self.conv4_0 = VGGBlock(nb_channels[3], nb_channels[4], nb_channels[4])\n",
        "\n",
        "        self.conv0_1 = VGGBlock(nb_channels[0]+nb_channels[1], nb_channels[0], nb_channels[0])\n",
        "        self.conv1_1 = VGGBlock(nb_channels[1]+nb_channels[2], nb_channels[1], nb_channels[1])\n",
        "        self.conv2_1 = VGGBlock(nb_channels[2]+nb_channels[3], nb_channels[2], nb_channels[2])\n",
        "        self.conv3_1 = VGGBlock(nb_channels[3]+nb_channels[4], nb_channels[3], nb_channels[3])\n",
        "        self.conv0_2 = VGGBlock(nb_channels[0]*2+nb_channels[1], nb_channels[0], nb_channels[0])\n",
        "        self.conv1_2 = VGGBlock(nb_channels[1]*2+nb_channels[2], nb_channels[1], nb_channels[1])\n",
        "        self.conv2_2 = VGGBlock(nb_channels[2]*2+nb_channels[3], nb_channels[2], nb_channels[2])\n",
        "        self.conv0_3 = VGGBlock(nb_channels[0]*3+nb_channels[1], nb_channels[0], nb_channels[0])\n",
        "        self.conv1_3 = VGGBlock(nb_channels[1]*3+nb_channels[2], nb_channels[1], nb_channels[1])\n",
        "        self.conv0_4 = VGGBlock(nb_channels[0]*4+nb_channels[1], nb_channels[0], nb_channels[0])\n",
        "\n",
        "\n",
        "        self.final = nn.Conv2d(nb_channels[0], output_size, kernel_size=1)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        x0_0 = self.conv0_0(input)\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
        "\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
        "\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
        "\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
        "\n",
        "\n",
        "        output = self.final(x0_4)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noIXCbWO8HzP"
      },
      "outputs": [],
      "source": [
        "model=Unetplus()\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NageUewZ6R1j"
      },
      "outputs": [],
      "source": [
        "# Number of parameters of our model\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fymh0v8R80G8"
      },
      "outputs": [],
      "source": [
        "learning_rate=1e-4\n",
        "\n",
        "criterion=nn.L1Loss().cuda()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPpYO6mh8_AD"
      },
      "outputs": [],
      "source": [
        "def print_summary(epoch, i, nb_batch, loss, batch_time, \n",
        "                  average_loss, average_time, mode):\n",
        "    '''\n",
        "        Print the losses and the computing time during the training or the validation\n",
        "    '''\n",
        "    summary = '[' + str(mode) + '] Epoch: [{0}][{1}/{2}]\\t'.format(\n",
        "        epoch, i, nb_batch)\n",
        "\n",
        "    string = ''\n",
        "    string += ('L1 Loss {:.4f} ').format(loss)\n",
        "    string += ('(Average {:.4f}) \\t').format(average_loss)\n",
        "    string += ('Batch Time {:.4f} ').format(batch_time)\n",
        "    string += ('(Average {:.4f}) \\t').format(average_time)\n",
        "\n",
        "    summary += string\n",
        "    print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADgU3cj89BgH"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "def train_loop(loader, model, criterion, optimizer, epoch):\n",
        "\n",
        "    logging_mode = 'Train' if model.training else 'Val'\n",
        "    if model.training:print('training')\n",
        "    \n",
        "    epoch_time_sum, epoch_loss_sum = [], []\n",
        "    \n",
        "    for i, sample in enumerate(loader, 1):\n",
        "        start = time.time()\n",
        "        # Take variable \n",
        "        (inp, dose) = sample\n",
        "\n",
        "        # Put variables to GPU\n",
        "        inp = inp.float().cuda()\n",
        "        dose = dose.float().cuda()\n",
        "    \n",
        "        # Compute model prediction\n",
        "        pred_dose = model(inp)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(pred_dose, dose)\n",
        "\n",
        "        # If in training mode ...\n",
        "        if model.training:\n",
        "            # Initialize optimizer gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            # Perform backpropagation\n",
        "            loss.backward()\n",
        "            # Update the model's trainable parameters using the computed gradients\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        # Compute elapsed time\n",
        "        batch_time = time.time() - start\n",
        "\n",
        "        epoch_time_sum += [batch_time]\n",
        "        epoch_loss_sum += [loss.item()]\n",
        "        \n",
        "        average_time = np.mean(epoch_time_sum)\n",
        "        average_loss = np.mean(epoch_loss_sum)\n",
        "\n",
        "\n",
        "        # Display the loss and the time \n",
        "        if i % print_frequency == 0:\n",
        "            print_summary(epoch + 1, i, len(loader), loss, batch_time,\n",
        "                          average_loss, average_time,logging_mode)\n",
        "        step = epoch*len(loader) + i\n",
        "            \n",
        "    return np.mean(epoch_loss_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a-B8QWI9Cyw"
      },
      "outputs": [],
      "source": [
        "epochs = 40\n",
        "print_frequency = 20\n",
        "train_loss,val_loss=[],[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XqpLkLQ9GFQ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):  #  Training loop\n",
        "    print('******** Epoch [{}/{}]  ********'.format(epoch+1, epochs+1))\n",
        "    model.train()\n",
        "    print('Training')\n",
        "    train_loss.append(train_loop(train_dataloader, model, criterion, optimizer, epochs))\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print('Validation')\n",
        "    with torch.no_grad():   # Disable gradient computatio\n",
        "        model.eval()        \n",
        "        val_loss.append(train_loop(val_dataloader, model, criterion, optimizer, epochs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDjMo4uLQQtI"
      },
      "outputs": [],
      "source": [
        "# Generate the predictions and save them\n",
        "dir='Unet++results'\n",
        "if not os.path.exists(\"./\"+dir):\n",
        "    os.makedirs(\"./\"+dir)\n",
        "for i,batch in enumerate(dataset_test):\n",
        "  img=model(batch[0].unsqueeze(0).float().cuda())[0][0]\n",
        "  time.sleep(0.1)\n",
        "  np.save('./'+dir+'/sample_'+str(9000+i)+'.npy',img.cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLpUyZ9NYnei"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHJmgZDyYowm"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
