{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlAfI8mCWAf3"
      },
      "source": [
        "<center><h2>ALTeGraD 2022<br>Lab Session 2: Transfer learning for NLP</h2> 27 / 10 / 2022<br> M. Kamal Eddine, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> CHAUVIN Paul\n",
        "\n",
        "</center>\n",
        "\n",
        "<br><br>\n",
        "In this lab we will:\n",
        "* Implement and pretrain a language model with transformer architecture.\n",
        "* Use the pretrained model (transfer learning) to perform a sentiment analysis task which consists of classifying some books reviews into positive and negative ones.\n",
        "* Compare the performance of the pretrained model to a model trained from scratch.\n",
        " <br>\n",
        " \n",
        "<b>The deadline for this lab is November 14, 2022 11:59 PM.</b> More details about the submission and the architecture for this lab can be found in the handout PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IqukuIe0Rb_c"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FF6fjkqgN39"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "p0cj9WkSFQwl"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        '''\n",
        "        ntokens: the size of vocabulary\n",
        "        nhid: the hidden dimension of the model.\n",
        "        We assume that embedding_dim = nhid\n",
        "        nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "        nhead: the number of heads in the multiheadattention models\n",
        "        dropout: the dropout value\n",
        "         '''\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.encoder = nn.Embedding(ntoken, nhid) # fill me, nhid = the dim_embed\n",
        "        self.pos_encoder = PositionalEncoding(nhid) #fill me, the PositionalEncoding class is implemented in the next cell\n",
        "        encoder_layers = nn.TransformerEncoderLayer(nhid, nhead, nhid, dropout=dropout) #fill me we assume nhid = d_model = dim_feedforward\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers) #fill me\n",
        "        self.nhid = nhid\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.nhid) \n",
        "        src = self.pos_encoder(src)     #fill me \n",
        "        output = self.transformer_encoder(src, src_mask)  #fill me\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, nhid, nclasses):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.decoder = nn.Linear(nhid, nclasses) #fill me\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        output = self.decoder(src)\n",
        "        return output\n",
        "    \n",
        "class Model (nn.Module):\n",
        "    def __init__(self, ntoken, nhead, nhid, nlayers, nclasses, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.base = TransformerModel(ntoken, nhead, nhid, nlayers)\n",
        "        self.classifier = ClassificationHead(nhid, nclasses)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # base model\n",
        "        x = self.base(src, src_mask) #fill me\n",
        "        # classifier model\n",
        "        output = self.classifier(x) #fill me\n",
        "        return output"
      ],
      "metadata": {
        "id": "YOpypKi9FzAZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kt2QQohaFZry"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, nhid, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, nhid)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, nhid, 2).float() * (-math.log(10000.0) / nhid)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfEYHJx2JW6l"
      },
      "source": [
        "Let's verify if our model works, by applying one inference step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rhb2gkUhJMR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754d1dd4-b97a-4ed9-8091-ef60fc5fd750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 100])\n"
          ]
        }
      ],
      "source": [
        "ntokens = 100 # the size of vocabulary\n",
        "nhid = 200  # hidden dimension\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)\n",
        "dummy_input = torch.tensor([[2, 6, 2, 5, 43, 21]]).to(device)\n",
        "src_mask = model.base.generate_square_subsequent_mask(1).to(device)\n",
        "out = model.forward(dummy_input,src_mask)\n",
        "\n",
        "print(out.shape) # is it the right shape?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i74NN897Fcit"
      },
      "source": [
        "## Vocabulary and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "5qjd26ghWuff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea0dc10-bb9e-480e-fad7-41e736d6ce66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 21:48:00--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577587 (564K) [text/plain]\n",
            "Saving to: ‘dict.txt.1’\n",
            "\n",
            "dict.txt.1          100%[===================>] 564.05K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-11-14 21:48:01 (123 MB/s) - ‘dict.txt.1’ saved [577587/577587]\n",
            "\n",
            "▁d 1\n",
            "es 1\n",
            "▁l 1\n",
            "en 1\n",
            "on 1\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
        "!head -5 dict.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "vFdH_-JeFbGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd3ba9e-87b3-44e0-e0c3-727c4ee036a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁trop\n"
          ]
        }
      ],
      "source": [
        "path_vocab = \"dict.txt\"\n",
        "token2ind = {\"<sos>\": 0, \"<pad>\": 1, \"<eos>\": 2, \"<oov>\": 3} # the 4 first indices are reserved to special tokens\n",
        "\n",
        "\n",
        "with open(path_vocab, \"r\") as f:\n",
        "    i=0\n",
        "    for idx, line in enumerate(f):\n",
        "        word = line.split()[0].strip()\n",
        "        token2ind[word] = idx + 4 #fill me\n",
        "\n",
        "ind2token = {v: k for k, v in token2ind.items()} #fill me\n",
        "\n",
        "print(ind2token[1111])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOExGODajN8p"
      },
      "source": [
        "### Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Y0jN-Ar9i5Q1"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_documents,\n",
        "        path_labels=None,\n",
        "        token2ind={},\n",
        "        max_len=512,\n",
        "        task=\"language_modeling\",\n",
        "    ):\n",
        "        self.task = task\n",
        "        self.max_len = max_len\n",
        "        self.token2ind = token2ind\n",
        "        self.documents = []\n",
        "        self.labels = []\n",
        "        with open(path_documents, \"r\") as f1:\n",
        "            for line in f1:\n",
        "                self.documents.append(line.strip())\n",
        "        if task == \"classification\":\n",
        "            with open(path_labels, \"r\") as f1:\n",
        "                for line in f1:\n",
        "                    self.labels.append(int(line.strip()))\n",
        "            assert len(self.labels) == len(self.documents)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence = self.documents[index].split()\n",
        "        if len(sequence) > self.max_len - 1:\n",
        "            sequence = sequence[: self.max_len - 1]\n",
        "        source_sequence = [token2ind[\"<sos>\"]] + [self.token2ind[tk] if tk in self.token2ind.keys() #fill me (constract the input sequence using token2ind, sequence and special tokens)\n",
        "                                    else self.token2ind[\"<oov>\"] for tk in sequence]         \n",
        "        if self.task == \"language_modeling\":\n",
        "            target = source_sequence[1:]\n",
        "            target.append(self.token2ind[\"<eos>\"])\n",
        "        elif self.task == \"classification\":\n",
        "            target = [self.labels[index]]\n",
        "        sample = {\n",
        "            \"source_sequence\": torch.tensor(source_sequence),\n",
        "            \"target\": torch.tensor(target),\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def MyCollator(batch):\n",
        "    source_sequences = pad_sequence(\n",
        "        #we use padding to match the length of the sequences in the same batch\n",
        "        [sample[\"source_sequence\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    target = pad_sequence(\n",
        "        [sample[\"target\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    return source_sequences, target.reshape(-1)\n",
        "\n",
        "\n",
        "def get_loader(\n",
        "    path_documents,\n",
        "    path_labels=None,\n",
        "    token2ind={},\n",
        "    max_len=512,\n",
        "    batch_size=32,\n",
        "    task=\"language_modeling\",\n",
        "):\n",
        "    dataset = Dataset(\n",
        "        path_documents,\n",
        "        path_labels=path_labels,\n",
        "        token2ind=token2ind,\n",
        "        max_len=512,\n",
        "        task=task,\n",
        "    )\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=MyCollator,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTns4lHrjUTa"
      },
      "source": [
        "## The Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4_jwosiLjRsS"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    path_data_train,\n",
        "    path_labels_train=None,\n",
        "    path_data_valid=None,\n",
        "    save_interval=-1,\n",
        "    log_interval=5,\n",
        "    task=\"language_modeling\",\n",
        "    batch_size=32,\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    ntokens = len(token2ind)\n",
        "    data_loader = get_loader(\n",
        "        path_data_train,\n",
        "        path_labels_train,\n",
        "        token2ind,\n",
        "        task=task,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    \n",
        "    losses = []\n",
        "    for idx, data in enumerate(data_loader): #step 1\n",
        "        optimizer.zero_grad()\n",
        "        src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(\n",
        "            device\n",
        "        )\n",
        "        input = data[0].to(device)\n",
        "        output = model(input, src_mask) #step 2\n",
        "        if task == 'classification':\n",
        "            #last vector only\n",
        "            output = output[-1,:,:] #fill me \n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        target = data[1] #fill me\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target) #fill me, Cross entropy check next cells\n",
        "        #fill me step 3\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
        "        #fill me step 4\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() \n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} steps | \"\n",
        "                \"loss {:5.5f} | ppl {:8.3f}\".format(\n",
        "                    epoch, idx, len(data_loader), cur_loss, math.exp(cur_loss),\n",
        "                )\n",
        "            )\n",
        "            losses.append(cur_loss)\n",
        "            total_loss = 0\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "pgf6BDB9jUr6"
      },
      "outputs": [],
      "source": [
        "ntokens = len(token2ind) #fill me # the size of vocabulary\n",
        "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "nclasses = 2 # for classification task only\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "u-OLy4KIkDwf"
      },
      "outputs": [],
      "source": [
        "# optimization parameters\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=token2ind['<pad>'])\n",
        "lr = 0.0003  # learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Bwh3n9xZQy4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17378e6b-27c5-450c-ff7b-e36cf2a97796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 21:48:01--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10146460 (9.7M) [text/plain]\n",
            "Saving to: ‘pretraining_subset.txt.2’\n",
            "\n",
            "\rpretraining_subset.   0%[                    ]       0  --.-KB/s               \rpretraining_subset. 100%[===================>]   9.68M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-11-14 21:48:02 (304 MB/s) - ‘pretraining_subset.txt.2’ saved [10146460/10146460]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
        "path_data_train = \"pretraining_subset.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "0m11g4ScjZaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e7952c-1ba4-413a-82b5-794b6a75b056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 3125 steps | loss 7.62373 | ppl 2046.177\n",
            "| epoch   1 |  1000/ 3125 steps | loss 6.86875 | ppl  961.742\n",
            "| epoch   1 |  1500/ 3125 steps | loss 6.54053 | ppl  692.651\n",
            "| epoch   1 |  2000/ 3125 steps | loss 6.34896 | ppl  571.896\n",
            "| epoch   1 |  2500/ 3125 steps | loss 6.20552 | ppl  495.476\n",
            "| epoch   1 |  3000/ 3125 steps | loss 6.11111 | ppl  450.839\n",
            "| epoch   2 |   500/ 3125 steps | loss 5.83077 | ppl  340.620\n",
            "| epoch   2 |  1000/ 3125 steps | loss 5.78402 | ppl  325.064\n",
            "| epoch   2 |  1500/ 3125 steps | loss 5.75167 | ppl  314.716\n",
            "| epoch   2 |  2000/ 3125 steps | loss 5.70790 | ppl  301.236\n",
            "| epoch   2 |  2500/ 3125 steps | loss 5.66356 | ppl  288.174\n",
            "| epoch   2 |  3000/ 3125 steps | loss 5.63648 | ppl  280.474\n"
          ]
        }
      ],
      "source": [
        "#pretraining on a tiny subset\n",
        "log_interval = 500\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs + 1): #5\n",
        "    train(\n",
        "        path_data_train,\n",
        "        save_interval=-1,\n",
        "       # task='language_modeling', # fill me\n",
        "        batch_size=16,\n",
        "        log_interval=log_interval,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeOM1dOvkO4e"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-BcBC6FSkMH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a86916-4cc6-4dbd-b45f-c09b63996b1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 21:53:03--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88093955 (84M) [application/octet-stream]\n",
            "Saving to: ‘pretrained_model_4layers.pt.2’\n",
            "\n",
            "pretrained_model_4l 100%[===================>]  84.01M   373MB/s    in 0.2s    \n",
            "\n",
            "2022-11-14 21:53:04 (373 MB/s) - ‘pretrained_model_4layers.pt.2’ saved [88093955/88093955]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens).to(device) \n",
        "\n",
        "#load the checkpoint\n",
        "checkpoint = torch.load('pretrained_model_4layers.pt') \n",
        "#load state dict\n",
        "model.load_state_dict(checkpoint['model_state_dict']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "tBRRVsWqlIoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1eaef4d-4e83-4d57-acc1-d02f1b6c5f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "--2022-11-14 21:53:07--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115362 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘sentencepiece.french.model.2’\n",
            "\n",
            "sentencepiece.frenc 100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-11-14 21:53:07 (144 MB/s) - ‘sentencepiece.french.model.2’ saved [1115362/1115362]\n",
            "\n",
            "['▁Bonjour', '▁les', '▁amis', '!']\n",
            "Bonjour les amis!\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece   # uncomment this if you are using google colab\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "s = spm.SentencePieceProcessor(model_file='sentencepiece.french.model') #load sentencepiece model\n",
        "\n",
        "#examples\n",
        "encoded = s.encode_as_pieces(\"Bonjour les amis!\")\n",
        "decoded = s.decode_pieces(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "TtLlV05pkQI3"
      },
      "outputs": [],
      "source": [
        "def infer_next_token(sent):\n",
        "    model.eval()\n",
        "    sent_pieces = s.encode_as_pieces(sent)\n",
        "    source = [token2ind['<sos>']] + [token2ind[el] for el in sent_pieces] # list of tokens\n",
        "    source = torch.tensor(source).to(device)\n",
        "    source = source.reshape(-1, 1)\n",
        "    src_mask = model.base.generate_square_subsequent_mask(source.size(0)).to(device)\n",
        "    out = model(source, src_mask)\n",
        "    next_token_ind = torch.argmax(out,dim=2)[-1] #fill me\n",
        "    return next_token_ind, out\n",
        "    \n",
        "def infer_next_tokens(sent, max_len=50):\n",
        "    # to be implemented\n",
        "    next_token_ind, out = infer_next_token(sent)\n",
        "    next_token = s.decode_pieces(ind2token[next_token_ind.item()])\n",
        "\n",
        "    while (len(next_token_ind) < max_len and next_token != '<eos>'):\n",
        "      sent = \" \".join([sent, next_token]) \n",
        "      next_token_ind, out = infer_next_token(sent)\n",
        "      next_token = s.decode_pieces(ind2token[next_token_ind.item()])\n",
        "    \n",
        "    return \" \".join([sent, next_token]) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "f83Nn5nSly4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18d03c7c-8c1e-41be-e796-c9546be70b2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bonjour les gens qui ont été très accueillants et sympathiques . <eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "sent = \"Bonjour les\"\n",
        "infer_next_tokens(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7mjVzomoZ3"
      },
      "source": [
        "### Supervised task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "0K1BZsblmEmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04320d84-99be-4d43-e29c-e8356baff8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 21:53:07--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1495960 (1.4M) [text/plain]\n",
            "Saving to: ‘train.review.spm’\n",
            "\n",
            "\rtrain.review.spm      0%[                    ]       0  --.-KB/s               \rtrain.review.spm    100%[===================>]   1.43M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2022-11-14 21:53:07 (160 MB/s) - ‘train.review.spm’ saved [1495960/1495960]\n",
            "\n",
            "--2022-11-14 21:53:08--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200 (3.1K) [text/plain]\n",
            "Saving to: ‘train.label’\n",
            "\n",
            "train.label         100%[===================>]   3.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-14 21:53:08 (33.1 MB/s) - ‘train.label’ saved [3200/3200]\n",
            "\n",
            "--2022-11-14 21:53:08--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1864544 (1.8M) [text/plain]\n",
            "Saving to: ‘test.review.spm’\n",
            "\n",
            "test.review.spm     100%[===================>]   1.78M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2022-11-14 21:53:08 (189 MB/s) - ‘test.review.spm’ saved [1864544/1864544]\n",
            "\n",
            "--2022-11-14 21:53:08--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4000 (3.9K) [text/plain]\n",
            "Saving to: ‘test.label’\n",
            "\n",
            "test.label          100%[===================>]   3.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-14 21:53:08 (62.2 MB/s) - ‘test.label’ saved [4000/4000]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
        "\n",
        "path_data_train = \"train.review.spm\"\n",
        "path_labels_train = \"train.label\"\n",
        "\n",
        "path_data_valid = \"test.review.spm\"\n",
        "path_labels_valid = \"test.label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "_MLfvjiom2SL"
      },
      "outputs": [],
      "source": [
        "# a function to evaluate the validation accuracy of the model.\n",
        "def evaluate_accuracy(data_loader):\n",
        "    #to be implemented\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    ncorrect = ntotal = 0\n",
        "    with torch.no_grad():\n",
        "        # loop over the validation set\n",
        "        for data in data_loader:\n",
        "            # model output \n",
        "            src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(device)\n",
        "            output = model(data[0].to(device),src_mask)[-1,:,:]\n",
        "            output = output.view(-1, output.shape[-1])\n",
        "\n",
        "            # class inference \n",
        "            predictions = torch.argmax(output,dim=-1) \n",
        "\n",
        "            # total number of examples\n",
        "            ntotal += output.shape[0]\n",
        "\n",
        "            # number of correct predictions \n",
        "            label = data[1].to(device)\n",
        "            \n",
        "            ncorrect += torch.sum(predictions == label).item() # number of correct prediction \n",
        "        acc = ncorrect / ntotal\n",
        "        print(\"validation accuracy: {:3.2f}\".format(acc))\n",
        "        return acc    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "qzmx7T7xoa6v"
      },
      "outputs": [],
      "source": [
        "#save the base model to be loaded later in the fine-tuning phase\n",
        "torch.save({\"model_state_dict\": model.base.state_dict(),}, \"pretrained_model_4layers_no_class_head.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "i-xclMCpnVpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40099220-7792-4299-8637-49690cc6ddab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====Trainig FROM SCRATCH======\n",
            "| epoch   1 |    50/  200 steps | loss 0.84645 | ppl    2.331\n",
            "| epoch   1 |   100/  200 steps | loss 0.76069 | ppl    2.140\n",
            "| epoch   1 |   150/  200 steps | loss 0.76470 | ppl    2.148\n",
            "validation accuracy: 0.50\n",
            "| epoch   2 |    50/  200 steps | loss 0.75829 | ppl    2.135\n",
            "| epoch   2 |   100/  200 steps | loss 0.72854 | ppl    2.072\n",
            "| epoch   2 |   150/  200 steps | loss 0.73082 | ppl    2.077\n",
            "validation accuracy: 0.62\n",
            "| epoch   3 |    50/  200 steps | loss 0.74639 | ppl    2.109\n",
            "| epoch   3 |   100/  200 steps | loss 0.66401 | ppl    1.943\n",
            "| epoch   3 |   150/  200 steps | loss 0.66299 | ppl    1.941\n",
            "validation accuracy: 0.68\n",
            "| epoch   4 |    50/  200 steps | loss 0.49379 | ppl    1.639\n",
            "| epoch   4 |   100/  200 steps | loss 0.52342 | ppl    1.688\n",
            "| epoch   4 |   150/  200 steps | loss 0.46315 | ppl    1.589\n",
            "validation accuracy: 0.76\n",
            "| epoch   5 |    50/  200 steps | loss 0.20629 | ppl    1.229\n",
            "| epoch   5 |   100/  200 steps | loss 0.29904 | ppl    1.349\n",
            "| epoch   5 |   150/  200 steps | loss 0.15829 | ppl    1.172\n",
            "validation accuracy: 0.77\n",
            "| epoch   6 |    50/  200 steps | loss 0.08300 | ppl    1.087\n",
            "| epoch   6 |   100/  200 steps | loss 0.11799 | ppl    1.125\n",
            "| epoch   6 |   150/  200 steps | loss 0.12529 | ppl    1.133\n",
            "validation accuracy: 0.77\n",
            "| epoch   7 |    50/  200 steps | loss 0.05520 | ppl    1.057\n",
            "| epoch   7 |   100/  200 steps | loss 0.01312 | ppl    1.013\n",
            "| epoch   7 |   150/  200 steps | loss 0.03367 | ppl    1.034\n",
            "validation accuracy: 0.76\n",
            "| epoch   8 |    50/  200 steps | loss 0.04739 | ppl    1.049\n",
            "| epoch   8 |   100/  200 steps | loss 0.00138 | ppl    1.001\n",
            "| epoch   8 |   150/  200 steps | loss 0.01891 | ppl    1.019\n",
            "validation accuracy: 0.74\n",
            "| epoch   9 |    50/  200 steps | loss 0.04097 | ppl    1.042\n",
            "| epoch   9 |   100/  200 steps | loss 0.00194 | ppl    1.002\n",
            "| epoch   9 |   150/  200 steps | loss 0.05151 | ppl    1.053\n",
            "validation accuracy: 0.76\n",
            "| epoch  10 |    50/  200 steps | loss 0.02466 | ppl    1.025\n",
            "| epoch  10 |   100/  200 steps | loss 0.01906 | ppl    1.019\n",
            "| epoch  10 |   150/  200 steps | loss 0.02572 | ppl    1.026\n",
            "validation accuracy: 0.76\n",
            "| epoch  11 |    50/  200 steps | loss 0.01796 | ppl    1.018\n",
            "| epoch  11 |   100/  200 steps | loss 0.04251 | ppl    1.043\n",
            "| epoch  11 |   150/  200 steps | loss 0.02080 | ppl    1.021\n",
            "validation accuracy: 0.77\n",
            "| epoch  12 |    50/  200 steps | loss 0.00166 | ppl    1.002\n",
            "| epoch  12 |   100/  200 steps | loss 0.00921 | ppl    1.009\n",
            "| epoch  12 |   150/  200 steps | loss 0.00519 | ppl    1.005\n",
            "validation accuracy: 0.77\n",
            "| epoch  13 |    50/  200 steps | loss 0.00009 | ppl    1.000\n",
            "| epoch  13 |   100/  200 steps | loss 0.02120 | ppl    1.021\n",
            "| epoch  13 |   150/  200 steps | loss 0.00011 | ppl    1.000\n",
            "validation accuracy: 0.77\n",
            "| epoch  14 |    50/  200 steps | loss 0.02294 | ppl    1.023\n",
            "| epoch  14 |   100/  200 steps | loss 0.00072 | ppl    1.001\n",
            "| epoch  14 |   150/  200 steps | loss 0.00013 | ppl    1.000\n",
            "validation accuracy: 0.76\n",
            "| epoch  15 |    50/  200 steps | loss 0.01680 | ppl    1.017\n",
            "| epoch  15 |   100/  200 steps | loss 0.02579 | ppl    1.026\n",
            "| epoch  15 |   150/  200 steps | loss 0.00012 | ppl    1.000\n",
            "validation accuracy: 0.74\n",
            "\n",
            "=====PRETRAINED MODEL======\n",
            "| epoch   1 |    50/  200 steps | loss 0.98448 | ppl    2.676\n",
            "| epoch   1 |   100/  200 steps | loss 0.84832 | ppl    2.336\n",
            "| epoch   1 |   150/  200 steps | loss 0.77016 | ppl    2.160\n",
            "validation accuracy: 0.64\n",
            "| epoch   2 |    50/  200 steps | loss 0.68973 | ppl    1.993\n",
            "| epoch   2 |   100/  200 steps | loss 0.64520 | ppl    1.906\n",
            "| epoch   2 |   150/  200 steps | loss 0.61326 | ppl    1.846\n",
            "validation accuracy: 0.77\n",
            "| epoch   3 |    50/  200 steps | loss 0.48914 | ppl    1.631\n",
            "| epoch   3 |   100/  200 steps | loss 0.56585 | ppl    1.761\n",
            "| epoch   3 |   150/  200 steps | loss 0.46482 | ppl    1.592\n",
            "validation accuracy: 0.78\n",
            "| epoch   4 |    50/  200 steps | loss 0.42315 | ppl    1.527\n",
            "| epoch   4 |   100/  200 steps | loss 0.38296 | ppl    1.467\n",
            "| epoch   4 |   150/  200 steps | loss 0.42022 | ppl    1.522\n",
            "validation accuracy: 0.77\n",
            "| epoch   5 |    50/  200 steps | loss 0.32778 | ppl    1.388\n",
            "| epoch   5 |   100/  200 steps | loss 0.35757 | ppl    1.430\n",
            "| epoch   5 |   150/  200 steps | loss 0.39718 | ppl    1.488\n",
            "validation accuracy: 0.79\n",
            "| epoch   6 |    50/  200 steps | loss 0.35210 | ppl    1.422\n",
            "| epoch   6 |   100/  200 steps | loss 0.40081 | ppl    1.493\n",
            "| epoch   6 |   150/  200 steps | loss 0.33652 | ppl    1.400\n",
            "validation accuracy: 0.78\n",
            "| epoch   7 |    50/  200 steps | loss 0.26103 | ppl    1.298\n",
            "| epoch   7 |   100/  200 steps | loss 0.24681 | ppl    1.280\n",
            "| epoch   7 |   150/  200 steps | loss 0.27704 | ppl    1.319\n",
            "validation accuracy: 0.79\n",
            "| epoch   8 |    50/  200 steps | loss 0.13687 | ppl    1.147\n",
            "| epoch   8 |   100/  200 steps | loss 0.20037 | ppl    1.222\n",
            "| epoch   8 |   150/  200 steps | loss 0.31678 | ppl    1.373\n",
            "validation accuracy: 0.78\n",
            "| epoch   9 |    50/  200 steps | loss 0.18847 | ppl    1.207\n",
            "| epoch   9 |   100/  200 steps | loss 0.17367 | ppl    1.190\n",
            "| epoch   9 |   150/  200 steps | loss 0.18936 | ppl    1.208\n",
            "validation accuracy: 0.77\n",
            "| epoch  10 |    50/  200 steps | loss 0.15582 | ppl    1.169\n",
            "| epoch  10 |   100/  200 steps | loss 0.06593 | ppl    1.068\n",
            "| epoch  10 |   150/  200 steps | loss 0.25652 | ppl    1.292\n",
            "validation accuracy: 0.78\n",
            "| epoch  11 |    50/  200 steps | loss 0.12656 | ppl    1.135\n",
            "| epoch  11 |   100/  200 steps | loss 0.09584 | ppl    1.101\n",
            "| epoch  11 |   150/  200 steps | loss 0.14226 | ppl    1.153\n",
            "validation accuracy: 0.78\n",
            "| epoch  12 |    50/  200 steps | loss 0.13630 | ppl    1.146\n",
            "| epoch  12 |   100/  200 steps | loss 0.10667 | ppl    1.113\n",
            "| epoch  12 |   150/  200 steps | loss 0.13746 | ppl    1.147\n",
            "validation accuracy: 0.79\n",
            "| epoch  13 |    50/  200 steps | loss 0.02523 | ppl    1.026\n",
            "| epoch  13 |   100/  200 steps | loss 0.13024 | ppl    1.139\n",
            "| epoch  13 |   150/  200 steps | loss 0.09118 | ppl    1.095\n",
            "validation accuracy: 0.78\n",
            "| epoch  14 |    50/  200 steps | loss 0.14090 | ppl    1.151\n",
            "| epoch  14 |   100/  200 steps | loss 0.10620 | ppl    1.112\n",
            "| epoch  14 |   150/  200 steps | loss 0.12144 | ppl    1.129\n",
            "validation accuracy: 0.78\n",
            "| epoch  15 |    50/  200 steps | loss 0.07642 | ppl    1.079\n",
            "| epoch  15 |   100/  200 steps | loss 0.03376 | ppl    1.034\n",
            "| epoch  15 |   150/  200 steps | loss 0.00588 | ppl    1.006\n",
            "validation accuracy: 0.78\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from_scratch_settings = [True, False]\n",
        "\n",
        "from_scratch_valid_acc = []\n",
        "pretrained_valid_acc = []\n",
        "lr = 0.0001\n",
        "\n",
        "for from_scratch in from_scratch_settings:\n",
        "    model = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    if not from_scratch:\n",
        "        print(\"=====PRETRAINED MODEL======\")\n",
        "        #load checkpoint\n",
        "        checkpoint = torch.load(\"pretrained_model_4layers_no_class_head.pt\")\n",
        "        #load state dict\n",
        "        model.base.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"=====Trainig FROM SCRATCH======\")\n",
        "    epochs = 15\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(\n",
        "            path_data_train,\n",
        "            path_labels_train,\n",
        "            save_interval=-1,\n",
        "            task='classification',\n",
        "            batch_size=8,\n",
        "            log_interval=50,\n",
        "        )\n",
        "        acc = evaluate_accuracy(\n",
        "            get_loader(\n",
        "                path_data_valid,\n",
        "                path_labels_valid,\n",
        "                token2ind=token2ind,\n",
        "                batch_size=20,\n",
        "                task='classification',\n",
        "            )\n",
        "        )\n",
        "        if from_scratch:\n",
        "            from_scratch_valid_acc.append(acc)\n",
        "        else:\n",
        "            pretrained_valid_acc.append(acc)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "RCpBIdTHojm6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "edc53132-1559-49eb-a997-7306d21b3f66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f46edc24110>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#Visualize the accuracy\n",
        "plt.figure()\n",
        "plt.plot(from_scratch_valid_acc, label=\"from scratch\")\n",
        "plt.plot(pretrained_valid_acc, label=\"pretrained\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch \")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc=\"lower right\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.15 ('altegrad')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "1f3cfdeab8dd8f9900bd16266619de191cf0f5e09365d74b1fba1714dce58066"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}