{"cells":[{"cell_type":"markdown","metadata":{"id":"o0-jZL4ZVIau"},"source":["# KAGGLE COMPETITION RECVIS TP3\n","Paul CHAUVIN\n","\n","paulchauvin97@gmail.com"]},{"cell_type":"markdown","metadata":{"id":"Ejoz7AXdVEKN"},"source":["##Environment\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4149,"status":"ok","timestamp":1669755689023,"user":{"displayName":"chauvin Paul","userId":"16585062209247819581"},"user_tz":-60},"id":"XCEIHjcTU_yY","outputId":"731a3956-fa92-407d-e2d3-bd89f4173fb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: chainercv in /usr/local/lib/python3.7/dist-packages (0.13.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from chainercv) (7.1.2)\n","Requirement already satisfied: chainer\u003e=6.0 in /usr/local/lib/python3.7/dist-packages (from chainercv) (7.8.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from chainer\u003e=6.0-\u003echainercv) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from chainer\u003e=6.0-\u003echainercv) (3.8.0)\n","Requirement already satisfied: numpy\u003e=1.9.0 in /usr/local/lib/python3.7/dist-packages (from chainer\u003e=6.0-\u003echainercv) (1.21.6)\n","Requirement already satisfied: six\u003e=1.9.0 in /usr/local/lib/python3.7/dist-packages (from chainer\u003e=6.0-\u003echainercv) (1.15.0)\n","Requirement already satisfied: protobuf\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from chainer\u003e=6.0-\u003echainercv) (3.19.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from chainer\u003e=6.0-\u003echainercv) (57.4.0)\n"]}],"source":["import argparse\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","from tqdm import tqdm\n","import PIL.Image as Image\n","import zipfile\n","import torchvision\n","import pandas as pd\n","import json\n","!pip install chainercv\n","import chainercv\n","from chainercv.datasets import voc_bbox_label_names\n","from chainercv.links import FasterRCNNVGG16, SSD300, SSD512, YOLOv3, YOLOv2\n","from chainercv.utils import read_image\n","from chainercv.visualizations import vis_bbox"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1493,"status":"ok","timestamp":1669755690505,"user":{"displayName":"chauvin Paul","userId":"16585062209247819581"},"user_tz":-60},"id":"M5rllZ8zTXWQ","outputId":"73af3a07-9f36-457e-8e78-92a1051bc43e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1669755690506,"user":{"displayName":"chauvin Paul","userId":"16585062209247819581"},"user_tz":-60},"id":"iLOHp8KH--Vj"},"outputs":[],"source":["use_cuda = torch.cuda.is_available()\n","import sys\n","sys.argv=['']\n","del sys"]},{"cell_type":"markdown","metadata":{"id":"hLT-ggIb0Wqe"},"source":["##Crop images"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":36077,"status":"ok","timestamp":1669755726578,"user":{"displayName":"chauvin Paul","userId":"16585062209247819581"},"user_tz":-60},"id":"_4SgWSMMlsAp"},"outputs":[],"source":["path = '/content/drive/MyDrive/Cours_MVA/object_recognition/Kaggle_TP3/bird_dataset_cropped'\n","model4 = FasterRCNNVGG16(pretrained_model='voc07')\n","model0 = SSD300(pretrained_model='voc0712')\n","model2 = SSD512(pretrained_model='voc0712')\n","model6 = YOLOv3(pretrained_model='voc0712')\n","model7 = YOLOv2(pretrained_model='voc0712')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"lWX92ty2nrxJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["['mistery_category']\n","[]\n","0\n","50\n","100\n","150\n","200\n"]}],"source":["def save_img(bboxes,full_path):\n","    img = Image.open(full_path)\n","    top, left , bottom, right  = bboxes[0][0]\n","    cropped = img.crop( ( left, top, right, bottom) )  # size: 45, 45\n","    try:\n","        cropped.save(full_path, \"JPEG\", quality=80, optimize=True, progressive=True)\n","    except IOError:\n","        PIL.ImageFile.MAXBLOCK = cropped.size[0] * cropped.size[1]\n","        cropped.save(full_path, \"JPEG\", quality=80, optimize=True, progressive=True)\n","def comparer_model(model, img):\n","    bboxes0, labels0, scores0 = model.predict([img])\n","    if len(bboxes0[0])\u003e0:\n","        if labels0[0][0]==2:\n","            maximum_proba=scores0[0][0]\n","            return maximum_proba, bboxes0\n","    return(-1,-1)\n","def main_crop(path):\n","    for dossier, sous_dossiers, fichiers in os.walk(path):\n","        print (sous_dossiers)\n","        for num, fichier in enumerate(fichiers):\n","            if num%50==0:\n","                print(num)\n","            full_path = os.path.join(dossier, fichier)\n","            img = read_image(full_path)\n","            maximum0, bboxes_f0 = comparer_model(model0, img)\n","            maximum1, bboxes_f2 = comparer_model(model2, img)\n","            maximum2, bboxes_f4 = comparer_model(model4, img)\n","            maximum3, bboxes_f6 = comparer_model(model6, img)\n","            maximum4, bboxes_f7 = comparer_model(model7, img)\n","            \n","            if max([maximum0 ,maximum1, maximum2, maximum3, maximum4])\u003c=0.9:\n","                continue\n","            if maximum0==max([maximum0 ,maximum1, maximum2, maximum3, maximum4]):\n","                save_img(bboxes_f0,full_path)\n","                continue\n","            if maximum1==max([maximum1, maximum2, maximum3, maximum4]):\n","                save_img(bboxes_f2,full_path)\n","                continue\n","            if maximum2==max([maximum2, maximum3, maximum4]):\n","                save_img(bboxes_f4,full_path)\n","                continue\n","            if maximum3==max([maximum3, maximum4]):\n","                save_img(bboxes_f6,full_path)\n","                continue\n","            if maximum4\u003emax([maximum1, maximum2, maximum3, maximum4]):\n","                save_img(bboxes_f7,full_path)\n","                continue\n","main_crop(\"./bird_dataset_cropped/test_images\")\n","main_crop(\"./bird_dataset_cropped/train_images\")\n","main_crop(\"./bird_dataset_cropped/val_images\")\n","\n","def plot(full_path,model):\n","    # Read an RGB image and return it in CHW format.\n","    img = read_image(full_path)\n","    bboxes, labels, scores = model.predict([img])\n","    print(labels, scores)\n","    vis_bbox(img, bboxes[0], labels[0], scores[0],\n","             label_names=voc_bbox_label_names)\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEycnyoPn2B7"},"outputs":[],"source":["full_path1 = '//content/drive/MyDrive/Cours MVA/Object recognition/Kaggle_TP3/bird_dataset/test_images/mistery_category/0a4a063484eba8522de89bdb24878e74.jpg'\n","full_path2 = '//content/drive/MyDrive/Cours MVA/Object recognition/Kaggle_TP3/bird_dataset/test_images/mistery_category/0b7b64c3eeae20e8f71923914a8643ef.jpg'\n","full_path3 = '//content/drive/MyDrive/Cours MVA/Object recognition/Kaggle_TP3/bird_dataset/train_images/004.Groove_billed_Ani/Groove_Billed_Ani_0002_1670.jpg'\n","full_path4 = '//content/drive/MyDrive/Cours MVA/Object recognition/Kaggle_TP3/bird_dataset/train_images/009.Brewer_Blackbird/Brewer_Blackbird_0004_2345.jpg'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G71W-HxGnznm"},"outputs":[],"source":["plot(full_path1,model0)\n","plot(full_path1,model2)\n","plot(full_path1,model4)\n","plot(full_path1,model6)\n","plot(full_path1,model7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twBMKYzdnyza"},"outputs":[],"source":["plot(full_path2,model0)\n","plot(full_path2,model2)\n","plot(full_path2,model4)\n","plot(full_path2,model6)\n","plot(full_path2,model7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk6KaYbuqJ6b"},"outputs":[],"source":["plot(full_path3,model0)\n","plot(full_path3,model2)\n","plot(full_path3,model4)\n","plot(full_path3,model6)\n","plot(full_path3,model7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzrEpJ1JqKhD"},"outputs":[],"source":["plot(full_path4,model0)\n","plot(full_path4,model2)\n","plot(full_path4,model4)\n","plot(full_path4,model6)\n","plot(full_path4,model7)"]},{"cell_type":"markdown","metadata":{"id":"7WHtdRzS0eSP"},"source":["##Git Clone"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UyaEYbrDxN2J"},"outputs":[],"source":["!pip install --upgrade tf_slim\n","!pip install nets\n","!pip install preprocessing\n","%cd /content/drive/MyDrive/Cours_MVA/object_recognition/Kaggle_TP3\n","!git clone --recursive https://github.com/richardaecn/cvpr18-inaturalist-transfer.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rowgfc6S3sCc"},"outputs":[],"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import numpy as np\n","import os\n","import sys\n","import time\n","#import tensorflow as tf\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","import tf_slim as slim\n","sys.path.insert(0, './slim/nets/')\n","#import nets\n","import preprocessing\n","from slim.nets import inception_resnet_v2\n","from slim.nets import inception\n","from slim.preprocessing import inception_preprocessing\n","\n","data_dir = '/content/drive/MyDrive/Cours_MVA/object_recognition/Kaggle_TP3/cvpr18_inaturalist_transfer/data'\n","# dataset needs to be one of ['ILSVRC2012', 'inat2017', 'cub_200', 'flower_102',\n","# 'stanford_cars', 'stanford_dogs', 'aircraft', 'nabirds', 'food_101']\n","dataset = 'cub_200'\n","# base_network needs to be one of ['InceptionV3', 'InceptionV3SE',\n","# 'InceptionV4', 'InceptionResnetV2', 'InceptionResnetV2SE', 'ResNet50',\n","# 'ResNet101', 'ResNet152']\n","base_network = 'InceptionV3'\n","checkpoints_path ='/content/drive/MyDrive/Cours_MVA/object_recognition/Kaggle_TP3/cvpr18_inaturalist_transfer/inception_v3_iNat_299.ckpt'# './checkpoints/inception/inception_v3_iNat_299.ckpt'\n","# base_network = 'ResNet101'\n","# checkpoints_path = './checkpoints/resnet/resnet_101_ILSVRC_iNat_299.ckpt'\n","\n","image_size = 299\n","moving_average_decay = 0.9999\n","fea_dim = 2048\n","\n","# Read train and val list.\n","train_list = []\n","val_list = []\n","for line in open(os.path.join(data_dir, dataset, 'train.txt'), 'r'):\n","    train_list.append(\n","        (os.path.join(data_dir, dataset, line.strip().split(': ')[0]),\n","        int(line.strip().split(': ')[1])))\n","for line in open(os.path.join(data_dir, dataset, 'val.txt'), 'r'):\n","    val_list.append(\n","        (os.path.join(data_dir, dataset, line.strip().split(': ')[0]),\n","        int(line.strip().split(': ')[1])))\n","print('Length of train: %d' %len(train_list))\n","print('Length of val: %d' %len(val_list))\n","\n","# Base network architecture\n","if base_network == 'InceptionV3':\n","    endpoint = 'Mixed_7c'\n","    arg_scope = inception.inception_v3_arg_scope()\n","elif base_network == 'InceptionV3SE':\n","    endpoint = 'Mixed_7c'\n","    arg_scope = inception.inception_v3_se_arg_scope()\n","elif base_network == 'InceptionV4':\n","    endpoint = 'Mixed_7d'\n","    arg_scope = inception.inception_v4_arg_scope()\n","elif base_network == 'InceptionResnetV2':\n","    endpoint = 'Conv2d_7b_1x1'\n","    arg_scope = inception.inception_resnet_v2_arg_scope()\n","elif base_network == 'InceptionResnetV2SE':\n","    endpoint = 'Conv2d_7b_1x1'\n","    arg_scope = inception.inception_resnet_v2_se_arg_scope()\n","elif base_network[:6] == 'ResNet':\n","    layers = base_network.split('ResNet')[1]\n","    base_network = 'ResNet'\n","\n","# Feature extraction.\n","fea_train = np.zeros((len(train_list), fea_dim), dtype=np.float32)\n","label_train = np.zeros((len(train_list), ), dtype=np.int32)\n","fea_val = np.zeros((len(val_list), fea_dim), dtype=np.float32)\n","label_val = np.zeros((len(val_list), ), dtype=np.int32)\n","\n","with tf.Graph().as_default():\n","    tf_global_step = tf.train.get_or_create_global_step()\n","    image_path = tf.placeholder(tf.string)\n","    image = tf.image.decode_jpeg(tf.read_file(image_path), channels=3)\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","    image = inception_preprocessing.preprocess_image(image,\n","                                                     image_size,\n","                                                     image_size,\n","                                                     is_training=False)\n","    images  = tf.expand_dims(image, 0)\n","\n","    if base_network == 'ResNet':\n","        with slim.arg_scope(resnet_v2.resnet_arg_scope(use_batch_norm=True)):\n","            if layers == '50':\n","                net, _ = resnet_v2.resnet_v2_50(images, is_training=False)\n","            elif layers == '101':\n","                net, _ = resnet_v2.resnet_v2_101(images, is_training=False)\n","            elif layers == '152':\n","                net, _ = resnet_v2.resnet_v2_152(images, is_training=False)\n","    else:\n","        with slim.arg_scope(arg_scope):\n","            slim_args = [slim.batch_norm, slim.dropout]\n","            with slim.arg_scope(slim_args, is_training=False):\n","                with tf.variable_scope(base_network, reuse=None) as scope:\n","                    if base_network == 'InceptionV3':\n","                        net, _ = inception.inception_v3_base(\n","                            images, final_endpoint=endpoint, scope=scope)\n","                    elif base_network == 'InceptionV3SE':\n","                        net, _ = inception.inception_v3_se_base(\n","                            images, final_endpoint=endpoint, scope=scope)\n","                    elif base_network == 'InceptionV4':\n","                        net, _ = inception.inception_v4_base(\n","                            images, final_endpoint=endpoint, scope=scope)\n","                    elif base_network == 'InceptionResnetV2':\n","                        net, _ = inception.inception_resnet_v2_base(\n","                            images, final_endpoint=endpoint, scope=scope)\n","                    elif base_network == 'InceptionResnetV2SE':\n","                        net, _ = inception.inception_resnet_v2_se_base(\n","                            images, final_endpoint=endpoint, scope=scope)\n","    net = tf.reduce_mean(net, [0,1,2])\n","\n","    variable_averages = tf.train.ExponentialMovingAverage(\n","        moving_average_decay, tf_global_step)\n","    variables_to_restore = variable_averages.variables_to_restore()\n","    init_fn = slim.assign_from_checkpoint_fn(\n","        checkpoints_path, variables_to_restore)\n","\n","    config_sess = tf.ConfigProto(allow_soft_placement=True)\n","    config_sess.gpu_options.allow_growth = True\n","    with tf.Session(config=config_sess) as sess:\n","        init_fn(sess)\n","        start = time.time()\n","        print('Feature extraction on training set...')\n","        for i in range(len(fea_train)):\n","            if i%1000 == 0:\n","                print('%d/%d %.2fs'%(i, len(fea_train), time.time() - start))\n","            fea = sess.run(net, feed_dict={image_path:train_list[i][0]})\n","            fea_train[i, :] = fea\n","            label_train[i] = train_list[i][1]\n","        print('Feature extraction on validation set...')\n","        for i in range(len(fea_val)):\n","            if i%1000 == 0:\n","                print('%d/%d %.2fs'%(i, len(fea_val), time.time() - start))\n","            fea = sess.run(net, feed_dict={image_path:val_list[i][0]})\n","            fea_val[i, :] = fea\n","            label_val[i] = val_list[i][1]\n","\n","model_name = checkpoints_path.split('/')[-1].split('.ckpt')[0]\n","if not os.path.exists(os.path.join('./feature', model_name)):\n","    os.makedirs(os.path.join('./feature', model_name))\n","\n","save_dir = os.path.join('./feature', model_name, dataset)\n","np.save(os.path.join(save_dir + '_feature_train.npy'), fea_train)\n","np.save(os.path.join(save_dir + '_label_train.npy'), label_train)\n","np.save(os.path.join(save_dir + '_feature_val.npy'), fea_val)\n","np.save(os.path.join(save_dir + '_label_val.npy'), label_val)\n"]},{"cell_type":"markdown","metadata":{"id":"CNRUPx65y_5A"},"source":["##Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgH4Hdu7y-Wp"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import json\n","from sklearn.linear_model import LogisticRegression\n","with open('/content/drive/MyDrive/Cours_MVA/object_recognition/Kaggle_TP3/config.json') as json_data_file:\n","    config = json.load(json_data_file)\n","\n","dictionnaire_correspondance={4:0, 9:1,10:2,11:3,12:4,13:5,14:6,15:7,16:8,19:9,20:10,21:11,23:12,26:13,28:14,29:15,\n","                             30:16,31:17,33:18,34:19}\n","\n","print(\"Start load data\")\n","features_train = np.load(os.path.join(config[\"load_dir\"]+ '/_feature_train_assigment.npy'))\n","labels_train = np.load(os.path.join(config[\"load_dir\"]+ '/_label_train_assigment.npy'))\n","features_val = np.load(os.path.join(config[\"load_dir\"]+ '/_feature_val_assigment.npy'))\n","features_test = np.load(os.path.join(config[\"load_dir\"]+ '/_feature_test_assigment.npy'))\n","labels_val = np.load(os.path.join(config[\"load_dir\"]+ '/_label_val_assigment.npy'))\n","features_train_crop = np.load(os.path.join(config[\"load_dir\"]+ '/_feature_train_assigment_crop.npy'))\n","features_val_crop = np.load(os.path.join(config[\"load_dir\"]+ '/_feature_val_assigment_crop.npy'))\n","features_test_crop = np.load(os.path.join(config[\"load_dir\"]+ '/_feature_test_assigment_crop.npy'))\n","\n","\n","if config[\"concatenate\"]:\n","    features_train = np.concatenate((features_train, features_train_crop), axis=1)\n","    features_val = np.concatenate((features_val, features_val_crop), axis=1)\n","    features_test = np.concatenate((features_test, features_test_crop), axis=1)\n","\n","\n","print(\"End load data\")\n","print(\"Start regression\")\n","LR = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=100)\n","LR.fit(features_train, labels_train)\n","print(\"End regression\")\n","\n","labels_pred = LR.predict(features_val)\n","num_class = 20\n","acc = np.zeros((num_class, num_class), dtype=np.float32)\n","for i in range(len(labels_val)):\n","    acc[dictionnaire_correspondance[labels_val[i]], dictionnaire_correspondance[labels_pred[i]]] += 1.0\n","\n","print('Accuracy on Validation: %f' % (sum([acc[i,i] for i in range(num_class)]) / len(labels_val)))\n","\n","print(\"Start Predict and create file csv\")\n","labels_pred = LR.predict(features_test)\n","df = pd.read_csv('kaggle_template.csv')\n","for dossier, sous_dossiers, fichiers in os.walk('bird_dataset/test_images/mistery_category'):\n","    for num, fichier in enumerate(fichiers):\n","        num_photo = df.loc[df['Id'] == fichier.split('.')[0]].index[0]\n","        df.Category[num_photo] = dictionnaire_correspondance[int(labels_pred[num])]\n","print(\"Succesfully wrote kaggle.csv, you can upload this file to the kaggle competition website\")\n","\n","df.to_csv('kaggle.csv',index=False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPJSi7njXPj6r8a0/+H0Tdu","mount_file_id":"1CYUiVmrqodFcx5eTzq2CSp8wKQhjAKKx","name":"","version":""},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}